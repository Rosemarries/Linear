{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import textdistance\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#import library <br>\n",
    "that are needed to run this project<br><br>\n",
    "\n",
    "run all this in your terminal if you don't have any library:<br>\n",
    "    > pip install re<br>\n",
    "    > pip install numpy<br>\n",
    "    > pip install pandas<br>\n",
    "    > pip install textdistance<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(file_name):\n",
    "    words = []\n",
    "    with open(file_name, \"r\") as f:\n",
    "        data = f.read()\n",
    "        data_lower = data.lower()\n",
    "        words = re.findall(r\"\\w+\", data_lower)\n",
    "    return words\n",
    "\n",
    "def process_freq(file_name):\n",
    "    freqs = []\n",
    "    with open(file_name, \"r\") as f:\n",
    "        data = f.read()\n",
    "        freqs = list(map(int, re.findall(r\"\\w+\", data)))\n",
    "    return freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_vocab():\n",
    "    word_l = process_data(\"words4.txt\")\n",
    "    vocab = set(word_l)\n",
    "    print(f\"The first 10 words in the text are : \\n{word_l[0:10]}\")\n",
    "    print(f\"There are {len(vocab)} words in the vocabulary.\\n\")\n",
    "    return word_l\n",
    "\n",
    "def get_count(word_l, word, freq=[]):\n",
    "    word_count_dict = {}\n",
    "    for i in range(len(word_l)):\n",
    "        word_count_dict[word_l[i]] = 1\n",
    "    word_count_dict = Counter(word_l)\n",
    "    print(f\"There are {len(word_count_dict)} key values pairs\")\n",
    "    print(f\"The count for the word {word} is {word_count_dict.get(word, 0)}\")\n",
    "    return word_count_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_probabilities(word_count_dict):\n",
    "    probabilities = {}\n",
    "    total = sum(word_count_dict.values())\n",
    "    for i in word_count_dict:\n",
    "        probabilities[i] = float(\"{:f}\".format(word_count_dict[i] / total))\n",
    "    print(f\"Length of probabilities is {len(probabilities)}\")\n",
    "    # print(f\"P(\\\"{word}\\\") is {probabilities[word]:.4f}\\n\")\n",
    "    return probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first 10 words in the text are : \n",
      "['the', 'of', 'and', 'to', 'a', 'in', 'for', 'is', 'on', 'that']\n",
      "There are 10000 words in the vocabulary.\n",
      "\n",
      "There are 10000 key values pairs\n",
      "The count for the word linaar is 0\n",
      "Length of probabilities is 10000\n"
     ]
    }
   ],
   "source": [
    "word = input(\"Enter word : \")\n",
    "\n",
    "word_l = save_vocab()\n",
    "# freqs = process_freq(\"freq.txt\")\n",
    "word_count_dict = get_count(word_l, word)\n",
    "probabilities = get_probabilities(word_count_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_letter(word, verbose=False):\n",
    "    delete_l = []\n",
    "    delete_l = [word[0:i] + word[i+1 : len(word)] for i in range(len(word))]\n",
    "    if verbose:\n",
    "        print(f\"input word = {word}, delete_l = {delete_l}\\n\")\n",
    "    return delete_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def switch_letter(word, verbose=False):\n",
    "    switch_l = []\n",
    "    word1 = list(word)\n",
    "    for i in range(len(word1)-1):\n",
    "        a1 = list(word1)\n",
    "        a1[i], a1[i+1] = a1[i+1], a1[i]\n",
    "        b = \"\".join(a1)\n",
    "        switch_l.append(b)\n",
    "    if word in switch_l:\n",
    "        switch_l.remove(word)\n",
    "    if verbose:\n",
    "        print(f\"Input word = {word}, switch_l = {switch_l}\\n\")\n",
    "    return switch_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_letter(word, verbose=False):\n",
    "    replace_l = []\n",
    "    replace_set = []\n",
    "    lower = string.ascii_lowercase\n",
    "    for i in range(len(word)):\n",
    "        temp = [word[0:i] + j + word[i+1:len(word)] for j in lower]\n",
    "        temp.remove(word)\n",
    "        replace_set.extend(temp)\n",
    "    replace_l = sorted(list(replace_set))\n",
    "    if verbose:\n",
    "        print(f\"Input word = {word}, replace_l = {replace_l}\\n\")\n",
    "    return replace_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_letter(word, verbose=False):\n",
    "    insert_l = []\n",
    "    lower = string.ascii_lowercase\n",
    "    for i in range(len(word)+1):\n",
    "        temp = [word[0:i] + j + word[i:len(word)] for j in lower]\n",
    "        insert_l.extend(temp)\n",
    "    if verbose:\n",
    "        print(f\"Input word = {word}, insert_l = {insert_l}\")\n",
    "        print(f\"len(insert_l) = {len(insert_l)}\\n\")\n",
    "    return insert_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edit_1_letter(word):\n",
    "    edit_1_set = set(delete_letter(word) + insert_letter(word) + replace_letter(word) + switch_letter(word))\n",
    "    return edit_1_set\n",
    "\n",
    "def edit_2_letters(word, allow_switches=True):\n",
    "    edit_2_set = set()\n",
    "    insert_letter1 = []\n",
    "    replace_letter1 = []\n",
    "    switch_letter1 = []\n",
    "    delete_letter1 = []\n",
    "    l = list(edit_1_letter(word))\n",
    "    temp = []\n",
    "    for i in l:\n",
    "        temp = delete_letter(i)\n",
    "        delete_letter1.extend(temp)\n",
    "    for i in l:\n",
    "        temp = replace_letter(i)\n",
    "        replace_letter1.extend(temp)\n",
    "    for i in l:\n",
    "        temp = switch_letter(i)\n",
    "        switch_letter1.extend(temp)\n",
    "    for i in l:\n",
    "        temp = insert_letter(i)\n",
    "        insert_letter1.extend(temp)\n",
    "    edit_2_set = set(replace_letter1 + switch_letter1 + delete_letter1 + insert_letter1)\n",
    "    return edit_2_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_corrections(word, probabilities, vocab, n=2, verbose=True):\n",
    "    suggestions = []\n",
    "    n_best = []\n",
    "    if word in vocab:\n",
    "        suggestions = word\n",
    "    elif len(edit_1_letter(word)) != 0:\n",
    "        suggestions = edit_1_letter(word).intersection(set(vocab))\n",
    "    elif len(edit_2_letters(word)) != 0:\n",
    "        suggestions = edit_2_letters(word).intersection(set(vocab))\n",
    "    else:\n",
    "        suggestions = word\n",
    "    best_words = {i:probabilities[i] for i in suggestions if i in probabilities}\n",
    "    best_words = [(k, v) for k, v in sorted(best_words.items(), key=lambda item: item[1], reverse=True)]\n",
    "    n_best = best_words[:n]\n",
    "    if verbose:\n",
    "        print(f\"Enter word : {word}, suggestions = {suggestions}\")\n",
    "    return n_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_edit_distance(source, target, insert_cost=1, delete_cost=1, replace_cost=2):\n",
    "    a, b, c = 0, 0, 0\n",
    "    d = []\n",
    "    len_src = len(source)\n",
    "    len_target = len(target)\n",
    "    Dimension = np.zeros((len_src+1, len_target+1), dtype=int)\n",
    "    for row in range(0, len_src+1):\n",
    "        Dimension[row, 0] = row\n",
    "    for col in range(0, len_target+1):\n",
    "        Dimension[0, col] = col\n",
    "    for row in range(1, len_src+1):\n",
    "        for col in range(1, len_target+1):\n",
    "            r_cost = replace_cost\n",
    "            if source[row-1] == target[col-1]:\n",
    "                r_cost = 0\n",
    "            a = Dimension[row-1, col] + delete_cost\n",
    "            b = Dimension[row, col-1] + insert_cost\n",
    "            c = Dimension[row-1, col-1] + r_cost\n",
    "            d = [a, b, c]\n",
    "            Dimension[row, col] = min(d)\n",
    "    minimum_edit_distance = Dimension[len_src, len_target]\n",
    "    return Dimension, minimum_edit_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity(word, word_l):\n",
    "    return [1-(textdistance.Jaccard(qval=2).distance(v,word)) for v in word_l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary(word, word_l, sim,  probs, min_edit=[]):\n",
    "    df = pd.DataFrame.from_dict(probs, orient='index').reset_index()\n",
    "    df = df.rename(columns={'index':'Word', 0:'Prob'})\n",
    "    df['Word'] = word_l\n",
    "    df['Similarity'] = sim\n",
    "    if min_edit:\n",
    "        df['Min Edit'] = min_edit\n",
    "    if not len(min_edit):\n",
    "        output = df.sort_values(['Similarity'], ascending=False).head()\n",
    "    else:\n",
    "        output = df.sort_values(['Min Edit', 'Similarity'], ascending=[True, False]).head(10)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "word 0: linear\n",
      "minimum edits = 2\n",
      "\n",
      "   #  l  i  n  e  a  r\n",
      "#  0  1  2  3  4  5  6\n",
      "l  1  0  1  2  3  4  5\n",
      "i  2  1  0  1  2  3  4\n",
      "n  3  2  1  0  1  2  3\n",
      "a  4  3  2  1  2  1  2\n",
      "a  5  4  3  2  3  2  3\n",
      "r  6  5  4  3  4  3  2\n",
      "--------------------------------------------------\n",
      "\n",
      "word 1: linda\n",
      "minimum edits = 3\n",
      "\n",
      "   #  l  i  n  d  a\n",
      "#  0  1  2  3  4  5\n",
      "l  1  0  1  2  3  4\n",
      "i  2  1  0  1  2  3\n",
      "n  3  2  1  0  1  2\n",
      "a  4  3  2  1  2  1\n",
      "a  5  4  3  2  3  2\n",
      "r  6  5  4  3  4  3\n",
      "--------------------------------------------------\n",
      "\n",
      "word 2: in\n",
      "minimum edits = 4\n",
      "\n",
      "   #  i  n\n",
      "#  0  1  2\n",
      "l  1  2  3\n",
      "i  2  1  2\n",
      "n  3  2  1\n",
      "a  4  3  2\n",
      "a  5  4  3\n",
      "r  6  5  4\n",
      "--------------------------------------------------\n",
      "\n",
      "word 3: line\n",
      "minimum edits = 4\n",
      "\n",
      "   #  l  i  n  e\n",
      "#  0  1  2  3  4\n",
      "l  1  0  1  2  3\n",
      "i  2  1  0  1  2\n",
      "n  3  2  1  0  1\n",
      "a  4  3  2  1  2\n",
      "a  5  4  3  2  3\n",
      "r  6  5  4  3  4\n",
      "--------------------------------------------------\n",
      "\n",
      "word 4: link\n",
      "minimum edits = 4\n",
      "\n",
      "   #  l  i  n  k\n",
      "#  0  1  2  3  4\n",
      "l  1  0  1  2  3\n",
      "i  2  1  0  1  2\n",
      "n  3  2  1  0  1\n",
      "a  4  3  2  1  2\n",
      "a  5  4  3  2  3\n",
      "r  6  5  4  3  4\n",
      "--------------------------------------------------\n",
      "\n",
      "word 5: la\n",
      "minimum edits = 4\n",
      "\n",
      "   #  l  a\n",
      "#  0  1  2\n",
      "l  1  0  1\n",
      "i  2  1  2\n",
      "n  3  2  3\n",
      "a  4  3  2\n",
      "a  5  4  3\n",
      "r  6  5  4\n",
      "--------------------------------------------------\n",
      "\n",
      "word 6: near\n",
      "minimum edits = 4\n",
      "\n",
      "   #  n  e  a  r\n",
      "#  0  1  2  3  4\n",
      "l  1  2  3  4  5\n",
      "i  2  3  4  5  6\n",
      "n  3  2  3  4  5\n",
      "a  4  3  4  3  4\n",
      "a  5  4  5  4  5\n",
      "r  6  5  6  5  4\n",
      "--------------------------------------------------\n",
      "\n",
      "word 7: na\n",
      "minimum edits = 4\n",
      "\n",
      "   #  n  a\n",
      "#  0  1  2\n",
      "l  1  2  3\n",
      "i  2  3  4\n",
      "n  3  2  3\n",
      "a  4  3  2\n",
      "a  5  4  3\n",
      "r  6  5  4\n",
      "--------------------------------------------------\n",
      "\n",
      "word 8: ar\n",
      "minimum edits = 4\n",
      "\n",
      "   #  a  r\n",
      "#  0  1  2\n",
      "l  1  2  3\n",
      "i  2  3  4\n",
      "n  3  4  5\n",
      "a  4  3  4\n",
      "a  5  4  5\n",
      "r  6  5  4\n",
      "--------------------------------------------------\n",
      "\n",
      "word 9: aa\n",
      "minimum edits = 4\n",
      "\n",
      "   #  a  a\n",
      "#  0  1  2\n",
      "l  1  2  3\n",
      "i  2  3  4\n",
      "n  3  4  5\n",
      "a  4  3  4\n",
      "a  5  4  3\n",
      "r  6  5  4\n",
      "--------------------------------------------------\n",
      "      Word    Prob  Similarity  Min Edit\n",
      "0   linear  0.0001    0.428571         2\n",
      "1    linda  0.0001    0.285714         3\n",
      "16  binary  0.0001    0.428571         4\n",
      "3     line  0.0001    0.333333         4\n",
      "4     link  0.0001    0.333333         4\n",
      "11  latina  0.0001    0.250000         4\n",
      "20  nascar  0.0001    0.250000         4\n",
      "2       in  0.0001    0.200000         4\n",
      "7       na  0.0001    0.200000         4\n",
      "8       ar  0.0001    0.200000         4\n"
     ]
    }
   ],
   "source": [
    "if word in word_l:\n",
    "    print(f\"We have {word} in our dictionary.\")\n",
    "else:\n",
    "    matrix, min_edit, df = [], [], []\n",
    "    for i in range(len(word_l)):\n",
    "        matrix_temp, min_edit_temp = min_edit_distance(word, word_l[i])\n",
    "        matrix.append(matrix_temp)\n",
    "        min_edit.append(min_edit_temp)\n",
    "        idx = list(\"#\" + word)\n",
    "        cols = list(\"#\" + word_l[i])\n",
    "        df_temp = pd.DataFrame(matrix_temp, index=idx, columns=cols)\n",
    "        df.append(df_temp)\n",
    "    \n",
    "    sim = similarity(word, word_l)\n",
    "    for i in range(len(min_edit)):\n",
    "        for j in range(len(min_edit)-1):\n",
    "            if min_edit[j] > min_edit[j+1]:\n",
    "                min_edit[j], min_edit[j+1] = min_edit[j+1], min_edit[j]\n",
    "                word_l[j], word_l[j+1] = word_l[j+1], word_l[j]\n",
    "                sim[j], sim[j+1] = sim[j+1], sim[j]\n",
    "                df[j], df[j+1] = df[j+1], df[j]\n",
    "\n",
    "    count = 0\n",
    "    limits = 10\n",
    "    for i in range(50):\n",
    "        if count == limits:\n",
    "            break\n",
    "        for j in range(len(word_l)):\n",
    "            if count == limits:\n",
    "                break\n",
    "            if i == min_edit[j]:\n",
    "                print(f\"\\nword {j}: {word_l[j]}\")\n",
    "                print(f\"minimum edits = {min_edit[j]}\\n\")\n",
    "                print(df[j])\n",
    "                print(\"-\"*50)\n",
    "                count += 1\n",
    "    \n",
    "    print(summary(word, word_l, sim, probabilities, min_edit))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
